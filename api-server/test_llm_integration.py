#!/usr/bin/env python3
"""
Test script to verify LLM integration in RAG service.
"""

import os
import sys
from dotenv import load_dotenv

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from rag_service import create_rag_service

def test_llm_integration():
    """Test the LLM integration."""
    print("🧪 Testing LLM Integration...")
    
    # Load environment variables
    load_dotenv()
    
    # Check if OpenAI API key is configured
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key or openai_key == "your-openai-api-key":
        print("❌ OpenAI API key not configured")
        print("Please set OPENAI_API_KEY in your .env file")
        return False
    
    print("✅ OpenAI API key found")
    
    try:
        # Create RAG service
        print("🔧 Creating RAG service...")
        rag_service = create_rag_service(
            use_azure_ai=False,  # Disable Azure for this test
            use_mongodb=False    # Use local files
        )
        
        # Test query
        test_query = "How many weeks is the AI bootcamp?"
        print(f"❓ Testing query: '{test_query}'")
        
        # Generate response
        response = rag_service.generate_response(
            query=test_query,
            top_k=3,
            min_similarity=0.15
        )
        
        print("✅ Response generated successfully!")
        print(f"📝 Answer: {response.answer}")
        print(f"🎯 Confidence: {response.confidence:.2f}")
        print(f"⏱️ Processing time: {response.processing_time:.2f}s")
        print(f"📚 Sources found: {len(response.sources)}")
        
        # Check if the answer looks like it was generated by LLM (not just copied chunks)
        if len(response.answer) > 100 and "Based on the available information" not in response.answer:
            print("✅ Answer appears to be LLM-generated (not just copied chunks)")
            return True
        else:
            print("⚠️ Answer might be using fallback text generation")
            return False
            
    except Exception as e:
        print(f"❌ Error during testing: {e}")
        return False

if __name__ == "__main__":
    success = test_llm_integration()
    if success:
        print("\n🎉 LLM integration test PASSED!")
    else:
        print("\n💥 LLM integration test FAILED!")
        sys.exit(1)
